{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a481ed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from joblib import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df7ebfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(\"../data/processed/df_train_1.csv\", index_col=0)\n",
    "df_2 = pd.read_csv(\"../data/processed/df_train_2.csv\", index_col=0)\n",
    "df_3 = pd.read_csv(\"../data/processed/df_train_3.csv\", index_col=0)\n",
    "df_4 = pd.read_csv(\"../data/processed/df_train_4.csv\", index_col=0)\n",
    "df_valid = pd.read_csv(\"../data/processed/df_valid.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3ea931",
   "metadata": {},
   "source": [
    "## log transformation on some (continuous) variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a972ce8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to preview the distribution of the (cont.) variables\n",
    "def plot_cont_var_dist(df):\n",
    "    cols = list(filter(lambda x: x.startswith(\"Variable\"), df.columns))\n",
    "    fig, axs = plt.subplots(len(cols)//4 + 1, 4, figsize=(20,20))\n",
    "    for i, col in enumerate(cols):\n",
    "        df[col].hist(ax=axs[i//4][i%4])\n",
    "        axs[i//4][i%4].set_title(f\"Histogram for {col}\")\n",
    "    plt.show()\n",
    "\n",
    "# define function to preview the distribution of the log (cont.) variables\n",
    "def plot_cont_log_var_dist(df):\n",
    "    cols = list(filter(lambda x: x.startswith(\"Variable\"), df.columns))\n",
    "    fig, axs = plt.subplots(len(cols)//4 + 1, 4, figsize=(20,20))\n",
    "    for i, col in enumerate(cols):\n",
    "        np.log(df[col]).hist(ax=axs[i//4][i%4])\n",
    "        axs[i//4][i%4].set_title(f\"Histogram for log({col})\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2905eb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transform(df, cols):\n",
    "    df.loc[:,cols] = df.loc[:,cols].apply(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c89011",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ccb7229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the variables\n",
    "sc_1 = StandardScaler()\n",
    "df_1.iloc[:,6:] = sc_1.fit_transform(df_1.iloc[:,6:])\n",
    "sc_2 = StandardScaler()\n",
    "df_2.iloc[:,6:] = sc_2.fit_transform(df_2.iloc[:,6:])\n",
    "sc_3 = StandardScaler()\n",
    "df_3.iloc[:,6:] = sc_3.fit_transform(df_3.iloc[:,6:])\n",
    "sc_4 = StandardScaler()\n",
    "df_4.iloc[:,6:] = sc_4.fit_transform(df_4.iloc[:,6:])\n",
    "sc = [sc_1, sc_2, sc_3, sc_4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba0b156",
   "metadata": {},
   "source": [
    "## Encoding the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83395ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the years in the period, only leaving Quarter, and one-hot encode\n",
    "def encode_quarter(df):\n",
    "    if \"Period\" in df.columns:\n",
    "        onehotenc = OneHotEncoder(drop='first').fit(np.array(['1', '2', '3', '4']).reshape(-1, 1))\n",
    "        df.loc[:,[\"Quarter2\", \"Quarter3\", \"Quarter4\"]] = onehotenc.transform(\n",
    "            df[\"Period\"].apply(lambda x: x[-1]).to_numpy().reshape(-1, 1)\n",
    "        ).toarray()\n",
    "        return onehotenc\n",
    "    \n",
    "quarter_enc_1 = encode_quarter(df_1)\n",
    "quarter_enc_2 = encode_quarter(df_2)\n",
    "quarter_enc_3 = encode_quarter(df_3)\n",
    "quarter_enc_4 = encode_quarter(df_4)\n",
    "quarter_enc = [quarter_enc_1, quarter_enc_2, quarter_enc_3, quarter_enc_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2655ee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode BR Code\n",
    "def encode_br_code(df):\n",
    "    if \"BR Code\" in df.columns:\n",
    "        onehotenc = OneHotEncoder(drop='first').fit(np.array(range(108)).reshape(-1, 1))\n",
    "        df.loc[:,[f\"BR Code {c}\" for c in range(1, 108)]] = onehotenc.transform(\n",
    "            df[\"BR Code\"].to_numpy().reshape(-1, 1)\n",
    "        ).toarray()\n",
    "        return onehotenc\n",
    "    \n",
    "br_enc_1 = encode_br_code(df_1)\n",
    "br_enc_2 = encode_br_code(df_2)\n",
    "br_enc_3 = encode_br_code(df_3)\n",
    "br_enc_4 = encode_br_code(df_4)\n",
    "br_enc = [br_enc_1, br_enc_2, br_enc_3, br_enc_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23a71a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode Country Code\n",
    "def encode_country_code(df):\n",
    "    if \"Country_Code\" in df.columns:\n",
    "        onehotenc = OneHotEncoder(drop='first').fit(np.array([0, 1, 2]).reshape(-1, 1))\n",
    "        df.loc[:,[\"Country_Code_1\", \"Country_Code_2\"]] = onehotenc.transform(\n",
    "            df[\"Country_Code\"].to_numpy().reshape(-1, 1)\n",
    "        ).toarray()\n",
    "        return onehotenc\n",
    "    \n",
    "country_enc_1 = encode_country_code(df_1)\n",
    "country_enc_2 = encode_country_code(df_2)\n",
    "country_enc_3 = encode_country_code(df_3)\n",
    "country_enc_4 = encode_country_code(df_4)\n",
    "country_enc = [country_enc_1, country_enc_2, country_enc_3, country_enc_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d276e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the outcome, risk rating\n",
    "risk_rating_enc = LabelEncoder()\n",
    "risk_rating_enc.fit(list(range(1, 16)) + [17])\n",
    "df_1[\"risk_rating\"] = risk_rating_enc.transform(df_1[\"risk_rating\"])\n",
    "df_2[\"risk_rating\"] = risk_rating_enc.transform(df_2[\"risk_rating\"])\n",
    "df_3[\"risk_rating\"] = risk_rating_enc.transform(df_3[\"risk_rating\"])\n",
    "df_4[\"risk_rating\"] = risk_rating_enc.transform(df_4[\"risk_rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6f9f105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove encoded and useless columns\n",
    "df_1 = df_1.drop(columns=[\"Country_Code\", \"BR Code\", \"Period\", \"Client\"])\n",
    "df_2 = df_2.drop(columns=[\"Country_Code\", \"BR Code\", \"Period\", \"Client\"])\n",
    "df_3 = df_3.drop(columns=[\"Country_Code\", \"BR Code\", \"Period\", \"Client\"])\n",
    "df_4 = df_4.drop(columns=[\"Country_Code\", \"BR Code\", \"Period\", \"Client\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20afe1a",
   "metadata": {},
   "source": [
    "## Comparing the 4 datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0685939a",
   "metadata": {},
   "source": [
    "#### Prepare the validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61955387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IterativeImputer] Completing matrix with shape (4444, 118)\n",
      "[IterativeImputer] Ending imputation round 1/18, elapsed time 0.26\n",
      "[IterativeImputer] Ending imputation round 2/18, elapsed time 0.53\n",
      "[IterativeImputer] Ending imputation round 3/18, elapsed time 0.80\n",
      "[IterativeImputer] Ending imputation round 4/18, elapsed time 1.03\n",
      "[IterativeImputer] Ending imputation round 5/18, elapsed time 1.26\n",
      "[IterativeImputer] Ending imputation round 6/18, elapsed time 1.50\n",
      "[IterativeImputer] Ending imputation round 7/18, elapsed time 1.74\n",
      "[IterativeImputer] Ending imputation round 8/18, elapsed time 1.98\n",
      "[IterativeImputer] Ending imputation round 9/18, elapsed time 2.20\n",
      "[IterativeImputer] Ending imputation round 10/18, elapsed time 2.46\n",
      "[IterativeImputer] Ending imputation round 11/18, elapsed time 2.68\n",
      "[IterativeImputer] Ending imputation round 12/18, elapsed time 2.93\n",
      "[IterativeImputer] Ending imputation round 13/18, elapsed time 3.18\n",
      "[IterativeImputer] Ending imputation round 14/18, elapsed time 3.43\n",
      "[IterativeImputer] Ending imputation round 15/18, elapsed time 3.66\n",
      "[IterativeImputer] Ending imputation round 16/18, elapsed time 3.88\n",
      "[IterativeImputer] Ending imputation round 17/18, elapsed time 4.12\n",
      "[IterativeImputer] Ending imputation round 18/18, elapsed time 4.37\n"
     ]
    }
   ],
   "source": [
    "# examine the four different training datasets (with different missing data handling)\n",
    "# by fitting a simple decision tree model and test.\n",
    "\n",
    "# load the files for feature selection and featuer engineering on the validation dataset\n",
    "with open(\"../references/mean_sd.npy\", \"rb\") as f:\n",
    "    mean, sd = np.load(f)\n",
    "simple_imputer = load(\"../references/simple_imputer.joblib\")\n",
    "reg_imputer = load(\"../references/regression_imputer.joblib\")\n",
    "selected_cols = []\n",
    "with open(\"../references/selected_features.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        selected_cols.append(df_valid.columns[:6].to_list() + line.strip().split(\",\"))\n",
    "\n",
    "# apply the same manipulations on the validation dataset to obtain the four sets of validation \n",
    "# data for the four training datasets\n",
    "def transform_and_encode(df, df_ind):\n",
    "    df_ind -= 1\n",
    "    df.iloc[:,6:] = sc[df_ind].transform(df.iloc[:,6:])\n",
    "    df.loc[:,[\"Quarter2\", \"Quarter3\", \"Quarter4\"]] = quarter_enc[df_ind].transform(\n",
    "            df[\"Period\"].apply(lambda x: x[-1]).to_numpy().reshape(-1, 1)).toarray()\n",
    "    df.loc[:,[f\"BR Code {c}\" for c in range(1, 108)]] = br_enc[df_ind].transform(\n",
    "            df[\"BR Code\"].to_numpy().reshape(-1, 1)).toarray()\n",
    "    df.loc[:,[\"Country_Code_1\", \"Country_Code_2\"]] = country_enc[df_ind].transform(\n",
    "            df[\"Country_Code\"].to_numpy().reshape(-1, 1)).toarray()\n",
    "    df.loc[:,\"risk_rating\"] = risk_rating_enc.transform(df[\"risk_rating\"])\n",
    "    return df.drop(columns=[\"Country_Code\", \"BR Code\", \"Period\", \"Client\"])\n",
    "\n",
    "df_valid = df_valid.iloc[(~(np.abs(df_valid.iloc[:,6:] - mean) > sd * 2).any(axis=1)).to_list(),:]\n",
    "\n",
    "df_valid_1 = df_valid.loc[:,selected_cols[0]].dropna().copy()\n",
    "df_valid_1 = transform_and_encode(df_valid_1, 1)\n",
    "\n",
    "df_valid_2 = df_valid.loc[:,selected_cols[1]].dropna().copy()\n",
    "df_valid_2 = transform_and_encode(df_valid_2, 2)\n",
    "\n",
    "df_valid_3 = df_valid.copy()\n",
    "df_valid_3.iloc[:,6:] = simple_imputer.transform(df_valid_3.iloc[:,6:])\n",
    "df_valid_3 = df_valid_3.loc[:,selected_cols[2]]\n",
    "df_valid_3 = transform_and_encode(df_valid_3, 3)\n",
    "\n",
    "df_valid_4 = df_valid.copy()\n",
    "df_valid_4.iloc[:,6:] = reg_imputer.transform(df_valid_4.iloc[:,6:])\n",
    "df_valid_4 = df_valid_4.loc[:,selected_cols[3]]\n",
    "df_valid_4 = transform_and_encode(df_valid_4, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9457bb7b",
   "metadata": {},
   "source": [
    "#### Use a simple decision tree model to assess the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62233509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c17a8ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model with 1st dataset: 0.17087967644084934\n",
      "model with 2nd dataset: 0.16594110115236876\n",
      "model with 3rd dataset: 0.1707920792079208\n",
      "model with 4th dataset: 0.16741674167416742\n"
     ]
    }
   ],
   "source": [
    "clf_1 = DecisionTreeClassifier(random_state=821)\n",
    "clf_1.fit(df_1.iloc[:,2:], df_1[\"risk_rating\"])\n",
    "y_pred_1 = clf_1.predict(df_valid_1.iloc[:,2:])\n",
    "print(f\"model with 1st dataset: {f1_score(df_valid_1['risk_rating'], y_pred_1, average='micro')}\")\n",
    "\n",
    "clf_2 = DecisionTreeClassifier(random_state=821)\n",
    "clf_2.fit(df_2.iloc[:,2:], df_2[\"risk_rating\"])\n",
    "y_pred_2 = clf_2.predict(df_valid_2.iloc[:,2:])\n",
    "print(f\"model with 2nd dataset: {f1_score(df_valid_2['risk_rating'], y_pred_2, average='micro')}\")\n",
    "\n",
    "clf_3 = DecisionTreeClassifier(random_state=821)\n",
    "clf_3.fit(df_3.iloc[:,2:], df_3[\"risk_rating\"])\n",
    "y_pred_3 = clf_3.predict(df_valid_3.iloc[:,2:])\n",
    "print(f\"model with 3rd dataset: {f1_score(df_valid_3['risk_rating'], y_pred_3, average='micro')}\")\n",
    "\n",
    "clf_4 = DecisionTreeClassifier(random_state=821)\n",
    "clf_4.fit(df_4.iloc[:,2:], df_4[\"risk_rating\"])\n",
    "y_pred_4 = clf_4.predict(df_valid_4.iloc[:,2:])\n",
    "print(f\"model with 4th dataset: {f1_score(df_valid_4['risk_rating'], y_pred_4, average='micro')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a76173",
   "metadata": {},
   "source": [
    "The last dataset (4th) gives the best f1 score, which measures the overall performance (precision and recall) of the model fitted. Here, `average='micro'` is used for evaluating the overall f1 score globally (instead of averaging the individual f1 score for each label), so that the weight of each label is considered.\n",
    "\n",
    "The 4th dataset (together with the corresponding method of handling missing data and selected features) is selected to perform the next step - model fitting and tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62df58f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IterativeImputer] Completing matrix with shape (4451, 118)\n",
      "[IterativeImputer] Ending imputation round 1/18, elapsed time 0.25\n",
      "[IterativeImputer] Ending imputation round 2/18, elapsed time 0.47\n",
      "[IterativeImputer] Ending imputation round 3/18, elapsed time 0.69\n",
      "[IterativeImputer] Ending imputation round 4/18, elapsed time 0.95\n",
      "[IterativeImputer] Ending imputation round 5/18, elapsed time 1.20\n",
      "[IterativeImputer] Ending imputation round 6/18, elapsed time 1.44\n",
      "[IterativeImputer] Ending imputation round 7/18, elapsed time 1.67\n",
      "[IterativeImputer] Ending imputation round 8/18, elapsed time 1.91\n",
      "[IterativeImputer] Ending imputation round 9/18, elapsed time 2.14\n",
      "[IterativeImputer] Ending imputation round 10/18, elapsed time 2.37\n",
      "[IterativeImputer] Ending imputation round 11/18, elapsed time 2.61\n",
      "[IterativeImputer] Ending imputation round 12/18, elapsed time 2.85\n",
      "[IterativeImputer] Ending imputation round 13/18, elapsed time 3.09\n",
      "[IterativeImputer] Ending imputation round 14/18, elapsed time 3.33\n",
      "[IterativeImputer] Ending imputation round 15/18, elapsed time 3.57\n",
      "[IterativeImputer] Ending imputation round 16/18, elapsed time 3.82\n",
      "[IterativeImputer] Ending imputation round 17/18, elapsed time 4.08\n",
      "[IterativeImputer] Ending imputation round 18/18, elapsed time 4.34\n"
     ]
    }
   ],
   "source": [
    "# manipulate the test dataset in the same way as the 4th train/validation dataset, \n",
    "# and save all of them as the finalized train/validation/test datasets.\n",
    "df_test = pd.read_csv(\"../data/processed/df_test.csv\", index_col=0)\n",
    "df_test = df_test.iloc[(~(np.abs(df_test.iloc[:,6:] - mean) > sd * 2).any(axis=1)).to_list(),:]\n",
    "df_test.iloc[:,6:] = reg_imputer.transform(df_test.iloc[:,6:])\n",
    "df_test = df_test.loc[:,selected_cols[3]]\n",
    "df_test = transform_and_encode(df_test, 4)\n",
    "\n",
    "df_4.to_csv(\"../data/processed/df_train_final.csv\")\n",
    "df_valid_4.to_csv(\"../data/processed/df_valid_final.csv\")\n",
    "df_test.to_csv(\"../data/processed/df_test_final.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:OLG]",
   "language": "python",
   "name": "conda-env-OLG-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
